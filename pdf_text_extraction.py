import fitz
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential
from dotenv import load_dotenv
import os
from typing import Optional
from tqdm import tqdm
import time

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
AZURE_FORM_KEY = os.getenv("AZURE_FORM_KEY")
AZURE_FORM_ENDPOINT = os.getenv("AZURE_FORM_ENDPOINT")

form_client = DocumentAnalysisClient(
    endpoint=AZURE_FORM_ENDPOINT,
    credential=AzureKeyCredential(AZURE_FORM_KEY)
)

llm = ChatOpenAI(
    openai_api_key=OPENAI_API_KEY,
    model="gpt-4o-mini", 
    temperature=0.7
)

OUTPUT_DIR = "output"
SPLIT_DIR = os.path.join(OUTPUT_DIR, "split_pages")
os.makedirs(SPLIT_DIR, exist_ok=True)

def split_pdf_per_page(pdf_path: str) -> list:
    doc = fitz.open(pdf_path)
    split_paths = []
    for i in range(len(doc)):
        new_doc = fitz.open()
        new_doc.insert_pdf(doc, from_page=i, to_page=i)
        split_path = os.path.join(SPLIT_DIR, f"page_{i + 1}.pdf")
        new_doc.save(split_path)
        new_doc.close()
        split_paths.append(split_path)
    return split_paths

def analyze_pdf_page_local(file_path: str, page_num: int, max_retries: int = 3, retry_delay: float = 2):
    for attempt in range(max_retries):
        try:
            with open(file_path, "rb") as f:
                poller = form_client.begin_analyze_document(
                    model_id="prebuilt-layout",
                    document=f
                )
                result = poller.result()

            page = result.pages[0]
            lines = [line.content for line in page.lines]
            extracted_text = "\n".join(lines)

            messages = [
                SystemMessage(content=(
                    "ã‚ãªãŸã¯å„ªç§€ãªæ”¿ç­–ã‚„æ³•æ¡ˆã®å†…å®¹ã‚’åˆ†æã—ã€ã‚ã‹ã‚Šã‚„ã™ãã¾ã¨ã‚ã‚‹å°‚é–€å®¶ã§ã™ã€‚\n\n"
                    "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã€æ”¿ç­–ã‚„æ³•æ¡ˆã®å†…å®¹ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚\n\n"
                    "ã‚ãªãŸã®ä»•äº‹ã¯ã€ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ã‹ã‚Šã‚„ã™ãã¾ã¨ã‚ã€Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n"
                    "å¥èª­ç‚¹ã‚„æ”¹è¡Œã®æ•´å‚™ã€è¡¨ã‚„é …ç›®ã®æ•´ç†ã‚‚è¡Œã£ã¦ãã ã•ã„ã€‚"
                )),
                HumanMessage(content=f'## ãƒšãƒ¼ã‚¸{page_num}\n\n###æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ\n\n{extracted_text}')
            ]

            response = llm.invoke(messages)
            return extracted_text, response.content

        except Exception as e:
            print(f"Error on page {page_num}, attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                return "", f"ã€æ•´å½¢å¤±æ•—ã€‘ãƒšãƒ¼ã‚¸{page_num}"
            time.sleep(retry_delay)

def extract_and_restructure_pdf(pdf_path: str, start_page: int = 1) -> None:
    print("ğŸ“„ å…¨ãƒšãƒ¼ã‚¸ã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¾ã™...")
    all_pages = split_pdf_per_page(pdf_path)
    pdf_pages = all_pages[start_page - 1:]

    raw_texts = []
    markdown_results = []

    for i, page_path in enumerate(tqdm(pdf_pages, desc="PDFãƒšãƒ¼ã‚¸å‡¦ç†ä¸­")):
        page_num = start_page + i
        extracted_text, markdown = analyze_pdf_page_local(page_path, page_num)
        raw_texts.append(f"## ãƒšãƒ¼ã‚¸{page_num}\n{extracted_text}")
        markdown_results.append(markdown)

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    with open(os.path.join(OUTPUT_DIR, "output.txt"), 'w', encoding='utf-8') as raw_file:
        raw_file.write('\n\n'.join(raw_texts))
    with open(os.path.join(OUTPUT_DIR, "output.md"), 'w', encoding='utf-8') as md_file:
        md_file.write('\n\n'.join(markdown_results))

    print("âœ… å‡¦ç†å®Œäº†ï¼output/output.txt ã¨ output/output.md ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

def process_all_pdfs_in_folder(folder_path: str):
    """
    æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€å†…ã®å…¨PDFã‚’ä¸€æ‹¬ã§å‡¦ç†ã—ã€output/é…ä¸‹ã«ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ»Markdownã‚’ä¿å­˜ã™ã‚‹
    """
    pdf_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.pdf')]
    if not pdf_files:
        print("PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    for pdf_file in pdf_files:
        pdf_path = os.path.join(folder_path, pdf_file)
        print(f"\n=== {pdf_file} ã®å‡¦ç†ã‚’é–‹å§‹ ===")
        all_pages = split_pdf_per_page(pdf_path)
        raw_texts = []
        markdown_results = []
        for i, page_path in enumerate(tqdm(all_pages, desc=f"{pdf_file}ã®ãƒšãƒ¼ã‚¸å‡¦ç†ä¸­")):
            page_num = i + 1
            extracted_text, markdown = analyze_pdf_page_local(page_path, page_num)
            raw_texts.append(f"## ãƒšãƒ¼ã‚¸{page_num}\n{extracted_text}")
            markdown_results.append(markdown)
        # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ™ãƒ¼ã‚¹ã§ä¿å­˜
        base_name = os.path.splitext(pdf_file)[0]
        with open(os.path.join(OUTPUT_DIR, f"{base_name}.txt"), 'w', encoding='utf-8') as raw_file:
            raw_file.write('\n\n'.join(raw_texts))
        with open(os.path.join(OUTPUT_DIR, f"{base_name}.md"), 'w', encoding='utf-8') as md_file:
            md_file.write('\n\n'.join(markdown_results))
        print(f"âœ… {pdf_file} ã®å‡¦ç†å®Œäº†ï¼output/{base_name}.txt, output/{base_name}.md ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n")

if __name__ == "__main__":
    # process_all_pdfs_in_folderã‚’å‘¼ã³å‡ºã™
    process_all_pdfs_in_folder("text")






#OPENAIã®ã¿
# def extract_and_restructure_pdf(
#         pdf_path: str,
#         output_md_path: str,
#         output_raw_text_path: str,
#         first_page:Optional[int] = 1,
#         last_page:Optional[int] = None,
#         max_retries:int = 3,
#         retry_delay:float = 2

# ) -> None:
#     """
#     PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ã€Markdownå½¢å¼ã§ä¿å­˜ã™ã‚‹

#      Args:
#             pdf_path (str): å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
#             output_md_path (str): æ•´å½¢çµæœã‚’ä¿å­˜ã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
#             output_raw_text_path (str): æŠ½å‡ºã•ã‚ŒãŸç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
#             first_page (Optional[int]): å‡¦ç†ã‚’é–‹å§‹ã™ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆ1å§‹ã¾ã‚Šï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1ã€‚
#             last_page (Optional[int]): å‡¦ç†ã‚’çµ‚äº†ã™ã‚‹ãƒšãƒ¼ã‚¸ç•ªå·ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ€çµ‚ãƒšãƒ¼ã‚¸ã€‚
#             max_retries (int): LLMå‘¼ã³å‡ºã—ã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯3ã€‚
#             retry_delay (int): LLMå‘¼ã³å‡ºã—å¤±æ•—æ™‚ã®ãƒªãƒˆãƒ©ã‚¤é–“éš”ï¼ˆç§’ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2ç§’ã€‚

#     """
#     #pdfã‚’é–‹ã
#     doc = fitz.open(pdf_path)
#     total_pages = len(doc)

#     #ãƒšãƒ¼ã‚¸ç¯„å›²ã®è¨ˆç®—
#     first_page = first_page or 1
#     last_page = last_page or total_pages

#     raw_texts = [] #ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
#     markdown_results = [] #æ•´å½¢å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ

#     #PDFãƒšãƒ¼ã‚¸ã‚’é †ã«å‡¦ç†
#     for page_num in tqdm(range(first_page - 1, last_page), desc="Processing PDF pages"):
#         page = doc[page_num]
#         extracted_text = page.get_text("text") #ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º

#         #LLMã¸ã®æŒ‡ç¤ºã‚’ä½œæˆ
#         messages = [
#             SystemMessage(content=(
#                 "ã‚ãªãŸã¯å„ªç§€ãªæ”¿ç­–ã‚„æ³•æ¡ˆã®å†…å®¹ã‚’åˆ†æã—ã€ã‚ã‹ã‚Šã‚„ã™ãã¾ã¨ã‚ã‚‹å°‚é–€å®¶ã§ã™ã€‚\n\n"
#                 "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã€æ”¿ç­–ã‚„æ³•æ¡ˆã®å†…å®¹ã‚’æŠ½å‡ºã—ãŸã‚‚ã®ã§ã™ã€‚\n\n"
#                 "ã‚ãªãŸã®ä»•äº‹ã¯ã€ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ã‹ã‚Šã‚„ã™ãã¾ã¨ã‚ã€Markdownå½¢å¼ã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã™ã€‚\n\n"
#                 "å‡ºåŠ›ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ã€‚\n\n"
#                 "1.å¥èª­ç‚¹ã‚„æ”¹è¡Œã®ä½ç½®ã‚’é©åˆ‡ã«æ•´ãˆã€èª¤å­—è„±å­—ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ï¼ˆæ–‡è„ˆã«åŸºã¥ãç¯„å›²å†…ã§ï¼‰ã€‚\n"
#                 "2. å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã«å«ã¾ã‚Œã‚‹æƒ…å ±ã‚’å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚\n"
#                 "3. è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã¯å¯èƒ½ãªé™ã‚Šå…ƒã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚\n"
#                 "4. ã‚°ãƒ©ãƒ•ã®è»¸ã®æ•°å€¤é–¢ä¿‚ã‚’ç¢ºèªã—ã€é©åˆ‡ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n\n"
#                 "æœ€çµ‚çµæœã¯Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
#             )),

#             HumanMessage(content=f'## ãƒšãƒ¼ã‚¸{page_num + 1}\n\n###æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ\n\n{extracted_text}')
#         ]

#         #LLMã¸ã®å‘¼ã³å‡ºã—ã‚’è©¦è¡Œ
#         for attempt in range(max_retries):
#             try:
#                 response = llm.invoke(messages)
#                 markdown_results.append(response.content)
#                 break
#             except Exception as e:
#                 print(f"Error during OpenAI API call on page {page_num + 1} (attempt {attempt + 1}): {e}")
#                 if attempt == max_retries - 1:
#                     print(f'Failed after max retries for page {page_num + 1}')
#                 time.sleep(retry_delay)

#         #æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
#         raw_texts.append(f'## ãƒšãƒ¼ã‚¸{page_num + 1}\n{extracted_text}')

#     #ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
#     with open(output_raw_text_path, 'w', encoding='utf-8') as raw_file:
#         raw_file.write('\n\n'.join(raw_texts))
#     print(f'æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_raw_text_path}')

#     #æ•´å½¢çµæœã‚’Markdownã§ä¿å­˜
#     with open(output_md_path, 'w', encoding='utf-8') as md_file:
#         md_file.write('\n\n'.join(markdown_results))
#     print(f'æ•´å½¢çµæœãŒMarkdownãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ: {output_md_path}')

# if __name__ == "__main__":
#     #ä½¿ç”¨ä¾‹
#     pdf_path = "03Hakusyo_part1_chap1_web.pdf" #å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
#     output_md_path = "output.md" #æ•´å½¢çµæœã®ä¿å­˜å…ˆ
#     output_raw_text_path = "output.txt" #æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ä¿å­˜å…ˆ

#     #å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
#     os.makedirs('output', exist_ok=True)

#     #PDFå‡¦ç†ã®å®Ÿè¡Œ
#     extract_and_restructure_pdf(
#         pdf_path=pdf_path,
#         output_md_path=output_md_path,
#         output_raw_text_path=output_raw_text_path,
#         first_page=None,
#         last_page=None
# )
